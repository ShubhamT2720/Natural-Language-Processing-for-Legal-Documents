{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShubhamT2720/Natural-Language-Processing-for-Legal-Documents/blob/main/Text_Sum_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nC4pNZGYr5_g",
        "outputId": "fb041428-e7a1-478a-bb6c-a81a96a6d829"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.32.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.5)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.82)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install sentencepiece\n",
        "!pip install pandas numpy\n",
        "!pip install nltk\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxO2F-BY5DGO",
        "outputId": "7b4ba20a-ce0e-4994-8f3a-52d829d71da5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = \"/content/drive/MyDrive/IN-Abs\"\n",
        "output_path = \"/content/drive/MyDrive/Output\""
      ],
      "metadata": {
        "id": "SXc6TqoPdKE1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "from nltk import tokenize\n",
        "import nltk\n",
        "import transformers\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "sys.path.insert(0, '../')\n",
        "import os\n"
      ],
      "metadata": {
        "id": "xpL2aM57bBS2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfcxnuoUgVqW",
        "outputId": "a9336f1a-eda5-4042-c87e-30c62eed95d8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "VDBTlee0fHNi"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(output_path):\n",
        "    os.makedirs(output_path)"
      ],
      "metadata": {
        "id": "TdVqCWcz2gRb"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import nltk\n",
        "\n",
        "def get_root_path():\n",
        "    '''\n",
        "    Function to get the root path of the dataset.\n",
        "    '''\n",
        "    # Ensure this path points to the root directory of your dataset.\n",
        "    path = \"/content/drive/MyDrive/IN-Abs\"\n",
        "    return path\n",
        "\n",
        "def get_summary_data(dataset, data_type):\n",
        "    '''\n",
        "    Function to get names, documents, and summaries.\n",
        "\n",
        "    Args:\n",
        "    - dataset: The name of the dataset (currently not used as there is only one dataset).\n",
        "    - data_type: Specifies whether to use 'train-data' or 'test-data'.\n",
        "\n",
        "    Returns:\n",
        "    - names: List of document names.\n",
        "    - data_source: List of document contents from judgment files.\n",
        "    - data_summary: List of summaries from summary files.\n",
        "    '''\n",
        "    # Set the path for the judgment and summary folders based on data_type.\n",
        "    path_judgment = os.path.join(get_root_path(), f'{data_type}/judgement')\n",
        "    path_summary = os.path.join(get_root_path(), f'{data_type}/summary')\n",
        "\n",
        "    # Load all judgment files.\n",
        "    all_judgment_files = glob.glob(path_judgment + \"/*.txt\")\n",
        "    data_source = []\n",
        "    names = []\n",
        "\n",
        "    for filename in all_judgment_files:\n",
        "        with open(filename, 'r') as f:\n",
        "            p = filename.rfind(\"/\")\n",
        "            names.append(filename[p+1:])  # Get the file name without the path.\n",
        "            a = f.read()\n",
        "            data_source.append(a)\n",
        "\n",
        "    # Load all summary files.\n",
        "    all_summary_files = glob.glob(path_summary + \"/*.txt\")\n",
        "    data_summary = []\n",
        "\n",
        "    for filename in all_summary_files:\n",
        "        with open(filename, 'r') as f:\n",
        "            a = f.read()\n",
        "            data_summary.append(a)\n",
        "\n",
        "    return names, data_source, data_summary\n",
        "\n",
        "def get_req_len_dict(dataset, data_type):\n",
        "    '''\n",
        "    Function to retrieve required length data for each summary.\n",
        "\n",
        "    Args:\n",
        "    - dataset: The name of the dataset (currently not used).\n",
        "    - data_type: Specifies whether to use 'train-data' or 'test-data'.\n",
        "\n",
        "    Returns:\n",
        "    - dict_names: A dictionary mapping document names to their required summary lengths.\n",
        "    '''\n",
        "    length_file_path = os.path.join(get_root_path(), f\"{data_type}/stats-IN-test.txt\")\n",
        "\n",
        "    with open(length_file_path, \"r\") as f:\n",
        "        a = f.read().split(\"\\n\")\n",
        "\n",
        "    dict_names = {}\n",
        "    for i in a:\n",
        "        b = i.split(\"\\t\")\n",
        "        try:\n",
        "            dict_names[b[0]] = int(b[1])\n",
        "        except IndexError:\n",
        "            print(f\"Error parsing line: {i}\")\n",
        "\n",
        "    return dict_names\n",
        "\n",
        "def split_to_sentences(para):\n",
        "    '''\n",
        "    Function to split a paragraph into sentences.\n",
        "\n",
        "    Args:\n",
        "    - para: A string containing a paragraph of text.\n",
        "\n",
        "    Returns:\n",
        "    - A list of sentences.\n",
        "    '''\n",
        "    sents = nltk.sent_tokenize(para)\n",
        "    return sents\n",
        "\n",
        "def nest_sentences(document, chunk_length):\n",
        "    '''\n",
        "    Function to chunk a document into nested sentences.\n",
        "\n",
        "    Args:\n",
        "    - document: The input document as a string.\n",
        "    - chunk_length: The maximum length of each chunk in words.\n",
        "\n",
        "    Returns:\n",
        "    - nested: A list of chunks, where each chunk is a list of sentences.\n",
        "    '''\n",
        "    nested = []\n",
        "    sent = []\n",
        "    length = 0\n",
        "\n",
        "    for sentence in nltk.sent_tokenize(document):\n",
        "        length += len(sentence.split(\" \"))\n",
        "        if length < chunk_length:\n",
        "            sent.append(sentence)\n",
        "        else:\n",
        "            nested.append(sent)\n",
        "            sent = [sentence]\n",
        "            length = len(sentence.split(\" \"))  # Reset length for the new chunk\n",
        "\n",
        "    if len(sent) > 0:\n",
        "        nested.append(sent)\n",
        "\n",
        "    return nested\n",
        "\n",
        "# Reading the test documents\n",
        "dataset = \"IN-Abs\"\n",
        "names, data_source, data_summary = get_summary_data(dataset, \"test-data\")\n",
        "print(f\"Number of names: {len(names)}\")\n",
        "print(f\"Number of documents: {len(data_source)}\")\n",
        "print(f\"Number of summaries: {len(data_summary)}\")\n",
        "\n",
        "# Getting the required summary lengths\n",
        "len_dic = dict_names = get_req_len_dict(dataset, \"test-data\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02pJxNNgcUpg",
        "outputId": "59e69e6e-1f5f-4ab3-ed7c-8d327b9a56da"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of names: 100\n",
            "Number of documents: 100\n",
            "Number of summaries: 100\n",
            "Error parsing line: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda:1\""
      ],
      "metadata": {
        "id": "myfS3M40eORj"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Automatically select device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nsi319/legal-pegasus\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"nsi319/legal-pegasus\").to(device)\n",
        "\n",
        "print(f\"Model loaded on device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pa_qDuu5fex4",
        "outputId": "6f3a5b15-33ca-4ad9-907d-523a75662a43"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded on device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize(text, max_len, min_len):\n",
        "    '''\n",
        "    Function to generate summary using Pegasus.\n",
        "\n",
        "    Args:\n",
        "    - text: The input text to be summarized.\n",
        "    - max_len: Maximum length of the summary.\n",
        "    - min_len: Minimum length of the summary.\n",
        "\n",
        "    Returns:\n",
        "    - summary: The generated summary.\n",
        "    '''\n",
        "    try:\n",
        "        input_tokenized = tokenizer.encode(text, return_tensors='pt', max_length=512, truncation=True).to(device)\n",
        "        summary_ids = model.generate(\n",
        "            input_tokenized,\n",
        "            num_beams=9,\n",
        "            length_penalty=0.1,\n",
        "            min_length=min_len,\n",
        "            max_length=max_len,\n",
        "        )\n",
        "        summary = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in summary_ids][0]\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        print(f\"Error during summarization: {e}\")\n",
        "        return \"\""
      ],
      "metadata": {
        "id": "Ni58QNRG697p"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_doc(nested_sentences, p):\n",
        "  result = []\n",
        "  for nested in nested_sentences:\n",
        "    # Join sentences directly\n",
        "    chunk_text = \" \".join(nested)\n",
        "    l = int(p * len(chunk_text.split(\" \")))\n",
        "    max_len = l + 10  # Allow some flexibility for truncation\n",
        "    min_len = l - 5\n",
        "    summary = summarize(chunk_text, max_len, min_len)\n",
        "    # Truncate while respecting sentence boundaries\n",
        "    sentences = sent_tokenize(summary)\n",
        "    truncated_summary = sentences[:min(len(sentences), l)]\n",
        "    result.append(\" \".join(truncated_summary))\n",
        "  return \" \".join(result)"
      ],
      "metadata": {
        "id": "1xXzpulPBqLT"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List done files once before the loop\n",
        "done_files = glob.glob(os.path.join(output_path, \"*.txt\"))\n",
        "done_files = [os.path.basename(f) for f in done_files]"
      ],
      "metadata": {
        "id": "yHWgyDBQloWK"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main loop to generate and save summaries of each document in the test dataset\n",
        "for i in range(min(5, len(data_source))):  # Limit to the first 5 documents\n",
        "    name = names[i]\n",
        "\n",
        "    # Skip if file has already been processed\n",
        "    if name in done_files:\n",
        "        continue\n",
        "\n",
        "    doc = data_source[i]\n",
        "    input_len = len(doc.split(\" \"))\n",
        "\n",
        "    # Check if name is in dict_names\n",
        "    if name not in dict_names:\n",
        "        print(f\"Warning: Required length for '{name}' not found in dict_names.\")\n",
        "        continue\n",
        "\n",
        "    req_len = dict_names[name]\n",
        "\n",
        "    # Print information for debugging\n",
        "    print(f\"{i}: {name} - {input_len} : {req_len}\", end=\", \")\n",
        "\n",
        "    # Avoid division by zero\n",
        "    if input_len == 0:\n",
        "        print(\"Error: Input length is zero, skipping this document.\")\n",
        "        continue\n",
        "\n",
        "    nested = nest_sentences(doc, 512)\n",
        "    p = float(req_len) / input_len\n",
        "    print(f\"p: {p}\")\n",
        "\n",
        "    abs_summ = summarize_doc(nested, p)\n",
        "    abs_summ = \" \".join(abs_summ.split())\n",
        "\n",
        "    # Print the length of the summary for debugging\n",
        "    print(f\"Summary length before truncation: {len(abs_summ.split(' '))}\")\n",
        "\n",
        "    # Ensure summary is not truncated mid-sentence\n",
        "    sentences = split_to_sentences(abs_summ)\n",
        "    if len(abs_summ.split(\" \")) > req_len:\n",
        "        abs_summ = \" \".join(sentences[:len(sentences)])\n",
        "\n",
        "    # Print the final length of the summary\n",
        "    print(f\"Final summary length: {len(abs_summ.split(' '))}\")\n",
        "\n",
        "    # Write the summary to a file\n",
        "    path = os.path.join(output_path, name)\n",
        "    try:\n",
        "        with open(path, 'w') as file:\n",
        "            file.write(abs_summ)\n",
        "    except IOError as e:\n",
        "        print(f\"Error writing to file '{path}': {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdtMB2QN7Zpv",
        "outputId": "df5840fb-3e07-4c20-903d-8b1d5f2ca834"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: 1181.txt - 3387 : 3510, p: 1.0363153232949514\n",
            "Summary length before truncation: 3137\n",
            "Final summary length: 3137\n",
            "1: 1195.txt - 4234 : 4389, p: 1.0366084081247047\n",
            "Summary length before truncation: 3912\n",
            "Final summary length: 3912\n",
            "2: 1329.txt - 2990 : 3083, p: 1.0311036789297658\n",
            "Summary length before truncation: 2818\n",
            "Final summary length: 2818\n",
            "3: 1378.txt - 2202 : 2281, p: 1.0358764759309718\n",
            "Summary length before truncation: 1992\n",
            "Final summary length: 1992\n",
            "4: 1406.txt - 2089 : 2165, p: 1.0363810435615126\n",
            "Summary length before truncation: 1973\n",
            "Final summary length: 1973\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}